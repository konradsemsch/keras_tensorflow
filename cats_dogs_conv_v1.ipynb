{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with multiple convolutional/ poolng layers of a capacity of 32/ 64 units each followed by a dense layer of capacity of 64 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size = (3, 3), activation = 'relu', input_shape = (150, 150, 3)), # size of the image + channels\n",
    "    keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "    keras.layers.Conv2D(64, kernel_size = (3, 3), activation = 'relu'),\n",
    "    keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "    keras.layers.Conv2D(128, kernel_size = (3, 3), activation = 'relu'),\n",
    "    keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "    keras.layers.Conv2D(128, kernel_size = (3, 3), activation = 'relu'),\n",
    "    keras.layers.MaxPool2D(pool_size = (2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.50), # adding strong dropout to prevent overfitting\n",
    "    keras.layers.Dense(512, activation = 'relu'),    \n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = 1e-4), # not sure which optimizer might be the best choice?\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna use the data image generator to read data batches one by one from disck instead of loading them all into memory at once, with some data augmentation techniques to prevent ovefitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1. / 255, \n",
    "    rotation_range = 40, # rotating the image randomly by a number of degrees to each of the side\n",
    "    width_shift_range = 0.2, # sliding the image as a fraction of its width or height\n",
    "    height_shift_range = 0.2, \n",
    "    shear_range = 0.2, # changing the angle of the image (shearing)\n",
    "    zoom_range = 0.2, # slightly zooming inside the image\n",
    "    horizontal_flip = True, # possible when there is no assumption of horizontal asymmetry\n",
    "    fill_mode = 'nearest' # the strategy of how newly created pixels will be filled e.g. after width/ height shift\n",
    ")\n",
    "\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1999 images belonging to 2 classes.\n",
      "Found 998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/Users/konrad/DS_ML/Python/SpecificEnv/keras_tensorflow/dogs-vs-cats-small/train'\n",
    "valid_dir = '/Users/konrad/DS_ML/Python/SpecificEnv/keras_tensorflow/dogs-vs-cats-small/valid'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size = (150, 150),\n",
    "        batch_size = 20,\n",
    "        class_mode = 'binary'\n",
    ")\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size = (150, 150),\n",
    "        batch_size = 20,\n",
    "        class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 240s 2s/step - loss: 0.6931 - accuracy: 0.5233 - val_loss: 0.6893 - val_accuracy: 0.5010\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 115s 1s/step - loss: 0.6876 - accuracy: 0.5258 - val_loss: 0.7038 - val_accuracy: 0.5070\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.6770 - accuracy: 0.5638 - val_loss: 0.6686 - val_accuracy: 0.5882\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.6663 - accuracy: 0.6033 - val_loss: 0.6477 - val_accuracy: 0.6062\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.6537 - accuracy: 0.6078 - val_loss: 0.6398 - val_accuracy: 0.6222\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.6469 - accuracy: 0.6183 - val_loss: 0.6257 - val_accuracy: 0.6273\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.6350 - accuracy: 0.6473 - val_loss: 0.6183 - val_accuracy: 0.6593\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.6244 - accuracy: 0.6538 - val_loss: 0.6030 - val_accuracy: 0.6693\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.6220 - accuracy: 0.6598 - val_loss: 0.6092 - val_accuracy: 0.6583\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 108s 1s/step - loss: 0.6080 - accuracy: 0.6528 - val_loss: 0.6657 - val_accuracy: 0.5852\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.6058 - accuracy: 0.6728 - val_loss: 0.5783 - val_accuracy: 0.6954\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.5760 - accuracy: 0.6983 - val_loss: 0.5804 - val_accuracy: 0.6764\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5957 - accuracy: 0.6713 - val_loss: 0.5786 - val_accuracy: 0.6904\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.5765 - accuracy: 0.6898 - val_loss: 0.5745 - val_accuracy: 0.6964\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5737 - accuracy: 0.7009 - val_loss: 0.5696 - val_accuracy: 0.6914\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5612 - accuracy: 0.7119 - val_loss: 0.5711 - val_accuracy: 0.6884\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5688 - accuracy: 0.7029 - val_loss: 0.5546 - val_accuracy: 0.7054\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5565 - accuracy: 0.7124 - val_loss: 0.5766 - val_accuracy: 0.7024\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.5431 - accuracy: 0.7229 - val_loss: 0.6225 - val_accuracy: 0.6613\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.5549 - accuracy: 0.7204 - val_loss: 0.5435 - val_accuracy: 0.7265\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 108s 1s/step - loss: 0.5253 - accuracy: 0.7364 - val_loss: 0.5447 - val_accuracy: 0.7134\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5485 - accuracy: 0.7164 - val_loss: 0.5984 - val_accuracy: 0.6613\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5411 - accuracy: 0.7259 - val_loss: 0.5339 - val_accuracy: 0.7285\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5332 - accuracy: 0.7354 - val_loss: 0.5254 - val_accuracy: 0.7345\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5166 - accuracy: 0.7489 - val_loss: 0.5391 - val_accuracy: 0.7285\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5344 - accuracy: 0.7254 - val_loss: 0.5199 - val_accuracy: 0.7425\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5203 - accuracy: 0.7444 - val_loss: 0.5281 - val_accuracy: 0.7244\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 108s 1s/step - loss: 0.5207 - accuracy: 0.7404 - val_loss: 0.5239 - val_accuracy: 0.7265\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: 0.5060 - accuracy: 0.7499 - val_loss: 0.5570 - val_accuracy: 0.7244\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: 0.5122 - accuracy: 0.7364 - val_loss: 0.5230 - val_accuracy: 0.7255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x142f31a10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch = 100, # 2000 samples, 20 per batches = 100 steps per each epoch\n",
    "    epochs = 30,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = 50 # 1000 samples, 20 per batch = 50 steps per each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/konrad/.local/share/virtualenvs/keras_tensorflow-hEZDQU6y/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: cats_dogs_conv_v1/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving the trained model\n",
    "# model.save('model_cats_dogs_conv_v1', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/1 - 3s - loss: 0.2210 - accuracy: 0.9186\n",
      "\n",
      "Test accuracy: 0.9186\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose = 2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.4292335e-22, 1.6219846e-22, 3.8224264e-22, 2.2197426e-20,\n",
       "       1.6082298e-26, 4.0185693e-12, 2.1265178e-23, 2.1613789e-11,\n",
       "       4.5142590e-21, 1.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_tensorflow",
   "language": "python",
   "name": "keras_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
